{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq5SV2cOETtp"
   },
   "source": [
    "Importing the textattack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uh0MlXXPEXw7",
    "outputId": "c8dcc9dc-77e7-4c87-a328-43d16dfdcfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textattack\n",
      "  Using cached textattack-0.3.10-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting bert-score>=0.3.5 (from textattack)\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting editdistance (from textattack)\n",
      "  Using cached editdistance-0.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting flair (from textattack)\n",
      "  Using cached flair-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from textattack)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting language-tool-python (from textattack)\n",
      "  Using cached language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting lemminflect (from textattack)\n",
      "  Using cached lemminflect-0.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting lru-dict (from textattack)\n",
      "  Using cached lru_dict-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting datasets>=2.4.0 (from textattack)\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting nltk (from textattack)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.21.0 (from textattack)\n",
      "  Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting pandas>=1.0.1 (from textattack)\n",
      "  Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy>=1.4.1 (from textattack)\n",
      "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting torch!=1.8,>=1.7.0 (from textattack)\n",
      "  Using cached torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers>=4.30.0 (from textattack)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting terminaltables (from textattack)\n",
      "  Using cached terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tqdm (from textattack)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting word2number (from textattack)\n",
      "  Using cached word2number-1.1-py3-none-any.whl\n",
      "Collecting num2words (from textattack)\n",
      "  Using cached num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting more-itertools (from textattack)\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting pinyin>=0.4.0 (from textattack)\n",
      "  Using cached pinyin-0.4.0-py3-none-any.whl\n",
      "Collecting jieba (from textattack)\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Collecting OpenHowNet (from textattack)\n",
      "  Using cached OpenHowNet-2.0-py3-none-any.whl.metadata (821 bytes)\n",
      "Requirement already satisfied: requests in ./venv_iris/lib/python3.8/site-packages (from bert-score>=0.3.5->textattack) (2.32.3)\n",
      "Collecting matplotlib (from bert-score>=0.3.5->textattack)\n",
      "  Using cached matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv_iris/lib/python3.8/site-packages (from bert-score>=0.3.5->textattack) (24.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.4.0->textattack)\n",
      "  Using cached pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.4.0->textattack)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets>=2.4.0->textattack)\n",
      "  Using cached xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.4.0->textattack)\n",
      "  Using cached multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.4.0->textattack)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.4.0->textattack)\n",
      "  Using cached aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets>=2.4.0->textattack)\n",
      "  Using cached huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv_iris/lib/python3.8/site-packages (from datasets>=2.4.0->textattack) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv_iris/lib/python3.8/site-packages (from pandas>=1.0.1->textattack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv_iris/lib/python3.8/site-packages (from pandas>=1.0.1->textattack) (2024.2)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.0.1->textattack)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv_iris/lib/python3.8/site-packages (from torch!=1.8,>=1.7.0->textattack) (4.12.2)\n",
      "Collecting sympy (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv_iris/lib/python3.8/site-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.30.0->textattack)\n",
      "  Using cached regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.30.0->textattack)\n",
      "  Using cached tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.30.0->textattack)\n",
      "  Using cached safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting boto3>=1.20.27 (from flair->textattack)\n",
      "  Using cached boto3-1.35.71-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting conllu<5.0.0,>=4.0 (from flair->textattack)\n",
      "  Using cached conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair->textattack)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair->textattack)\n",
      "  Using cached ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting gdown>=4.4.0 (from flair->textattack)\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langdetect>=1.0.9 (from flair->textattack)\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Collecting lxml>=4.8.0 (from flair->textattack)\n",
      "  Using cached lxml-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mpld3>=0.3 (from flair->textattack)\n",
      "  Using cached mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair->textattack)\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting scikit-learn>=1.0.2 (from flair->textattack)\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting segtok>=1.5.11 (from flair->textattack)\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair->textattack)\n",
      "  Using cached sqlitedict-2.1.0-py3-none-any.whl\n",
      "Collecting tabulate>=0.8.10 (from flair->textattack)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n",
      "  Using cached transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair->textattack)\n",
      "  Using cached Wikipedia_API-0.7.1-py3-none-any.whl\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n",
      "  Using cached semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting bioc<3.0.0,>=2.0.0 (from flair->textattack)\n",
      "  Using cached bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pip in ./venv_iris/lib/python3.8/site-packages (from language-tool-python->textattack) (24.3.1)\n",
      "Requirement already satisfied: wheel in ./venv_iris/lib/python3.8/site-packages (from language-tool-python->textattack) (0.45.0)\n",
      "Collecting click (from nltk->textattack)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->textattack)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting docopt>=0.6.2 (from num2words->textattack)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting anytree (from OpenHowNet->textattack)\n",
      "  Using cached anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: setuptools in ./venv_iris/lib/python3.8/site-packages (from OpenHowNet->textattack) (49.2.1)\n",
      "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair->textattack)\n",
      "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair->textattack)\n",
      "  Using cached intervaltree-3.1.0-py2.py3-none-any.whl\n",
      "Collecting botocore<1.36.0,>=1.35.71 (from boto3>=1.20.27->flair->textattack)\n",
      "  Using cached botocore-1.35.71-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair->textattack)\n",
      "  Using cached s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.13->flair->textattack)\n",
      "  Downloading wrapt-1.17.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv_iris/lib/python3.8/site-packages (from aiohttp->datasets>=2.4.0->textattack) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Using cached frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Using cached multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Using cached yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in ./venv_iris/lib/python3.8/site-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv_iris/lib/python3.8/site-packages (from gdown>=4.4.0->flair->textattack) (4.12.3)\n",
      "Requirement already satisfied: six in ./venv_iris/lib/python3.8/site-packages (from langdetect>=1.0.9->flair->textattack) (1.16.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert-score>=0.3.5->textattack)\n",
      "  Using cached contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert-score>=0.3.5->textattack)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert-score>=0.3.5->textattack)\n",
      "  Using cached fonttools-4.55.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->bert-score>=0.3.5->textattack)\n",
      "  Using cached kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib->bert-score>=0.3.5->textattack)\n",
      "  Using cached pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->bert-score>=0.3.5->textattack)\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv_iris/lib/python3.8/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (6.4.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv_iris/lib/python3.8/site-packages (from requests->bert-score>=0.3.5->textattack) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv_iris/lib/python3.8/site-packages (from requests->bert-score>=0.3.5->textattack) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv_iris/lib/python3.8/site-packages (from requests->bert-score>=0.3.5->textattack) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv_iris/lib/python3.8/site-packages (from requests->bert-score>=0.3.5->textattack) (2024.8.30)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=1.0.2->flair->textattack)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair->textattack)\n",
      "  Using cached sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting protobuf (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair->textattack)\n",
      "  Downloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv_iris/lib/python3.8/site-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch!=1.8,>=1.7.0->textattack)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->bert-score>=0.3.5->textattack)\n",
      "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv_iris/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->bert-score>=0.3.5->textattack) (3.20.2)\n",
      "Collecting accelerate>=0.26.0 (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack)\n",
      "  Using cached accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.4.0->textattack)\n",
      "  Using cached propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv_iris/lib/python3.8/site-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.6)\n",
      "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree->bioc<3.0.0,>=2.0.0->flair->textattack)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown>=4.4.0->flair->textattack)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: psutil in ./venv_iris/lib/python3.8/site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack) (6.1.0)\n",
      "Using cached textattack-0.3.10-py3-none-any.whl (445 kB)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Using cached torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached editdistance-0.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (402 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached flair-0.14.0-py3-none-any.whl (776 kB)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Using cached language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
      "Using cached lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
      "Using cached lru_dict-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached num2words-0.5.13-py3-none-any.whl (143 kB)\n",
      "Using cached OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
      "Using cached terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Using cached bioc-2.1-py3-none-any.whl (33 kB)\n",
      "Using cached boto3-1.35.71-py3-none-any.whl (139 kB)\n",
      "Using cached conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached lxml-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "Using cached matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "Using cached mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "Using cached multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "Using cached pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Using cached regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Using cached safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "Using cached scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Using cached semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached anytree-2.12.1-py3-none-any.whl (44 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached botocore-1.35.71-py3-none-any.whl (13.0 MB)\n",
      "Using cached contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.55.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Using cached frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Using cached kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Using cached pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Using cached s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Using cached sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading wrapt-1.17.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85 kB)\n",
      "Using cached yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Downloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Using cached propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: word2number, sqlitedict, sortedcontainers, sentencepiece, pptree, pinyin, mpmath, jieba, docopt, xxhash, wrapt, urllib3, tzdata, tqdm, threadpoolctl, terminaltables, tabulate, sympy, semver, safetensors, regex, PySocks, pyparsing, protobuf, propcache, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, num2words, networkx, multidict, more-itertools, lxml, lru-dict, langdetect, kiwisolver, jsonlines, joblib, jmespath, intervaltree, ftfy, fsspec, frozenlist, fonttools, filelock, editdistance, dill, cycler, conllu, click, async-timeout, anytree, aiohappyeyeballs, yarl, triton, segtok, scipy, pyarrow, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, lemminflect, deprecated, contourpy, botocore, bioc, aiosignal, wikipedia-api, scikit-learn, s3transfer, OpenHowNet, nvidia-cusolver-cu12, matplotlib, language-tool-python, huggingface-hub, aiohttp, torch, tokenizers, mpld3, gdown, boto3, transformers, pytorch-revgrad, datasets, accelerate, bert-score, transformer-smaller-training-vocab, flair, textattack\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed OpenHowNet-2.0 PySocks-1.7.1 accelerate-1.0.1 aiohappyeyeballs-2.4.3 aiohttp-3.10.11 aiosignal-1.3.1 anytree-2.12.1 async-timeout-5.0.1 bert-score-0.3.13 bioc-2.1 boto3-1.35.71 botocore-1.35.71 click-8.1.7 conllu-4.5.3 contourpy-1.1.1 cycler-0.12.1 datasets-3.1.0 deprecated-1.2.15 dill-0.3.8 docopt-0.6.2 editdistance-0.8.1 filelock-3.16.1 flair-0.14.0 fonttools-4.55.0 frozenlist-1.5.0 fsspec-2024.9.0 ftfy-6.2.3 gdown-5.2.0 huggingface-hub-0.26.3 intervaltree-3.1.0 jieba-0.42.1 jmespath-1.0.1 joblib-1.4.2 jsonlines-4.0.0 kiwisolver-1.4.7 langdetect-1.0.9 language-tool-python-2.8.1 lemminflect-0.2.3 lru-dict-1.3.0 lxml-5.3.0 matplotlib-3.7.5 more-itertools-10.5.0 mpld3-0.5.10 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.1 nltk-3.9.1 num2words-0.5.13 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 pandas-2.0.3 pillow-10.4.0 pinyin-0.4.0 pptree-3.1 propcache-0.2.0 protobuf-5.29.0 pyarrow-17.0.0 pyparsing-3.1.4 pytorch-revgrad-0.2.0 regex-2024.11.6 s3transfer-0.10.4 safetensors-0.4.5 scikit-learn-1.3.2 scipy-1.10.1 segtok-1.5.11 semver-3.0.2 sentencepiece-0.2.0 sortedcontainers-2.4.0 sqlitedict-2.1.0 sympy-1.13.3 tabulate-0.9.0 terminaltables-3.1.10 textattack-0.3.10 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.4.1 tqdm-4.67.1 transformer-smaller-training-vocab-0.4.0 transformers-4.46.3 triton-3.0.0 tzdata-2024.2 urllib3-1.26.20 wikipedia-api-0.7.1 word2number-1.1 wrapt-1.17.0 xxhash-3.5.0 yarl-1.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install textattack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9jLh395DeUC"
   },
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC-EZ2wADgiC"
   },
   "source": [
    "Train the default LSTM for 50 epochs on the\n",
    "Yelp Polarity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFLFdXhOFbUg",
    "outputId": "c90c7c2d-8e97-4fb9-a5ce-ed6040cbbcb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: [python -m] textattack <command> [<args>] train [-h]\n",
      "                                                       --model-name-or-path\n",
      "                                                       MODEL_NAME_OR_PATH\n",
      "                                                       [--model-max-length MODEL_MAX_LENGTH]\n",
      "                                                       [--model-num-labels MODEL_NUM_LABELS]\n",
      "                                                       [--attack ATTACK]\n",
      "                                                       [--task-type TASK_TYPE]\n",
      "                                                       --dataset DATASET\n",
      "                                                       [--dataset-train-split DATASET_TRAIN_SPLIT]\n",
      "                                                       [--dataset-eval-split DATASET_EVAL_SPLIT]\n",
      "                                                       [--filter-train-by-labels FILTER_TRAIN_BY_LABELS [FILTER_TRAIN_BY_LABELS ...]]\n",
      "                                                       [--filter-eval-by-labels FILTER_EVAL_BY_LABELS [FILTER_EVAL_BY_LABELS ...]]\n",
      "                                                       [--num-epochs NUM_EPOCHS]\n",
      "                                                       [--num-clean-epochs NUM_CLEAN_EPOCHS]\n",
      "                                                       [--attack-epoch-interval ATTACK_EPOCH_INTERVAL]\n",
      "                                                       [--early-stopping-epochs EARLY_STOPPING_EPOCHS]\n",
      "                                                       [--learning-rate LEARNING_RATE]\n",
      "                                                       [--num-warmup-steps NUM_WARMUP_STEPS]\n",
      "                                                       [--weight-decay WEIGHT_DECAY]\n",
      "                                                       [--per-device-train-batch-size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                                                       [--per-device-eval-batch-size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                                                       [--gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                                                       [--random-seed RANDOM_SEED]\n",
      "                                                       [--parallel]\n",
      "                                                       [--load-best-model-at-end]\n",
      "                                                       [--alpha ALPHA]\n",
      "                                                       [--num-train-adv-examples NUM_TRAIN_ADV_EXAMPLES]\n",
      "                                                       [--query-budget-train QUERY_BUDGET_TRAIN]\n",
      "                                                       [--attack-num-workers-per-device ATTACK_NUM_WORKERS_PER_DEVICE]\n",
      "                                                       [--output-dir OUTPUT_DIR]\n",
      "                                                       [--checkpoint-interval-steps CHECKPOINT_INTERVAL_STEPS]\n",
      "                                                       [--checkpoint-interval-epochs CHECKPOINT_INTERVAL_EPOCHS]\n",
      "                                                       [--save-last]\n",
      "                                                       [--log-to-tb]\n",
      "                                                       [--tb-log-dir TB_LOG_DIR]\n",
      "                                                       [--log-to-wandb]\n",
      "                                                       [--wandb-project WANDB_PROJECT]\n",
      "                                                       [--logging-interval-step LOGGING_INTERVAL_STEP]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model-name-or-path MODEL_NAME_OR_PATH, --model MODEL_NAME_OR_PATH\n",
      "                        Name or path of the model we want to create. \"lstm\"\n",
      "                        and \"cnn\" will create TextAttack's LSTM and CNN models\n",
      "                        while any other input will be used to create\n",
      "                        Transformers model. (e.g.\"brt-base-uncased\").\n",
      "                        (default: None)\n",
      "  --model-max-length MODEL_MAX_LENGTH\n",
      "                        The maximum sequence length of the model. (default:\n",
      "                        None)\n",
      "  --model-num-labels MODEL_NUM_LABELS\n",
      "                        The number of labels for classification. (default:\n",
      "                        None)\n",
      "  --attack ATTACK       Attack recipe to use (enables adversarial training)\n",
      "                        (default: None)\n",
      "  --task-type TASK_TYPE\n",
      "                        Type of task model is supposed to perform. Options:\n",
      "                        `classification`, `regression`. (default:\n",
      "                        classification)\n",
      "  --dataset DATASET     dataset for training; will be loaded from `datasets`\n",
      "                        library. if dataset has a subset, separate with a\n",
      "                        colon. ex: `glue^sst2` or `rotten_tomatoes` (default:\n",
      "                        yelp)\n",
      "  --dataset-train-split DATASET_TRAIN_SPLIT\n",
      "                        train dataset split, if non-standard (can\n",
      "                        automatically detect 'train' (default: )\n",
      "  --dataset-eval-split DATASET_EVAL_SPLIT\n",
      "                        val dataset split, if non-standard (can automatically\n",
      "                        detect 'dev', 'validation', 'eval') (default: )\n",
      "  --filter-train-by-labels FILTER_TRAIN_BY_LABELS [FILTER_TRAIN_BY_LABELS ...]\n",
      "                        List of labels to keep in the train dataset and\n",
      "                        discard all others. (default: None)\n",
      "  --filter-eval-by-labels FILTER_EVAL_BY_LABELS [FILTER_EVAL_BY_LABELS ...]\n",
      "                        List of labels to keep in the eval dataset and discard\n",
      "                        all others. (default: None)\n",
      "  --num-epochs NUM_EPOCHS, --epochs NUM_EPOCHS\n",
      "                        Total number of epochs for training. (default: 3)\n",
      "  --num-clean-epochs NUM_CLEAN_EPOCHS\n",
      "                        Number of epochs to train on the clean dataset before\n",
      "                        adversarial training (N/A if --attack unspecified)\n",
      "                        (default: 1)\n",
      "  --attack-epoch-interval ATTACK_EPOCH_INTERVAL\n",
      "                        Generate a new adversarial training set every N\n",
      "                        epochs. (default: 1)\n",
      "  --early-stopping-epochs EARLY_STOPPING_EPOCHS\n",
      "                        Number of epochs validation must increase before\n",
      "                        stopping early (-1 for no early stopping) (default:\n",
      "                        None)\n",
      "  --learning-rate LEARNING_RATE, --lr LEARNING_RATE\n",
      "                        Learning rate for Adam Optimization. (default: 5e-05)\n",
      "  --num-warmup-steps NUM_WARMUP_STEPS\n",
      "                        The number of steps for the warmup phase of linear\n",
      "                        scheduler. (default: 500)\n",
      "  --weight-decay WEIGHT_DECAY\n",
      "                        Weight decay (L2 penalty). (default: 0.01)\n",
      "  --per-device-train-batch-size PER_DEVICE_TRAIN_BATCH_SIZE\n",
      "                        The batch size per GPU/CPU for training. (default: 8)\n",
      "  --per-device-eval-batch-size PER_DEVICE_EVAL_BATCH_SIZE\n",
      "                        The batch size per GPU/CPU for evaluation. (default:\n",
      "                        32)\n",
      "  --gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate the gradients\n",
      "                        for, before performing a backward/update pass.\n",
      "                        (default: 1)\n",
      "  --random-seed RANDOM_SEED\n",
      "                        Random seed. (default: 786)\n",
      "  --parallel            If set, run training on multiple GPUs. (default:\n",
      "                        False)\n",
      "  --load-best-model-at-end\n",
      "                        If set, keep track of the best model across training\n",
      "                        and load it at the end. (default: False)\n",
      "  --alpha ALPHA         The weight of adversarial loss. (default: 1.0)\n",
      "  --num-train-adv-examples NUM_TRAIN_ADV_EXAMPLES\n",
      "                        The number of samples to attack when generating\n",
      "                        adversarial training set. Default is -1 (which is all\n",
      "                        possible samples). (default: -1)\n",
      "  --query-budget-train QUERY_BUDGET_TRAIN\n",
      "                        The max query budget to use when generating\n",
      "                        adversarial training set. (default: None)\n",
      "  --attack-num-workers-per-device ATTACK_NUM_WORKERS_PER_DEVICE\n",
      "                        Number of worker processes to run per device for\n",
      "                        attack. Same as `num_workers_per_device` argument for\n",
      "                        `AttackArgs`. (default: 1)\n",
      "  --output-dir OUTPUT_DIR\n",
      "                        Directory to output training logs and checkpoints.\n",
      "                        (default: ./outputs/2024-11-30-14-48-32-761446)\n",
      "  --checkpoint-interval-steps CHECKPOINT_INTERVAL_STEPS\n",
      "                        Save model checkpoint after every N updates to the\n",
      "                        model. (default: None)\n",
      "  --checkpoint-interval-epochs CHECKPOINT_INTERVAL_EPOCHS\n",
      "                        Save model checkpoint after every N epochs. (default:\n",
      "                        None)\n",
      "  --save-last           If set, save the model at end of training. Can be used\n",
      "                        with `--load-best-model-at-end` to save the best model\n",
      "                        at the end. (default: True)\n",
      "  --log-to-tb           If set, log to Tensorboard (default: False)\n",
      "  --tb-log-dir TB_LOG_DIR\n",
      "                        Path of Tensorboard log directory. (default: None)\n",
      "  --log-to-wandb        If set, log to Wandb. (default: False)\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        Name of Wandb project for logging. (default:\n",
      "                        textattack)\n",
      "  --logging-interval-step LOGGING_INTERVAL_STEP\n",
      "                        Log to Tensorboard/Wandb every N steps. (default: 1)\n"
     ]
    }
   ],
   "source": [
    "!textattack train --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKHhY1biDfqo",
    "outputId": "b103b175-a095-4248-d7e5-0717740f3a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mtextattack\u001b[0m: Loading textattack model: LSTMForClassification\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94myelp_polarity\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94myelp_polarity\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
      "\u001b[34;1mtextattack\u001b[0m: Writing logs to ./outputs/2024-11-30-15-08-48-546472/train_log.txt.\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote original training args to ./outputs/2024-11-30-15-08-48-546472/training_args.json.\n",
      "/mnt/aiongpfs/users/agross/venv_iris/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "\u001b[34;1mtextattack\u001b[0m: ***** Running training *****\n",
      "\u001b[34;1mtextattack\u001b[0m:   Num examples = 560000\n",
      "\u001b[34;1mtextattack\u001b[0m:   Num epochs = 50\n",
      "\u001b[34;1mtextattack\u001b[0m:   Num clean epochs = 50\n",
      "\u001b[34;1mtextattack\u001b[0m:   Instantaneous batch size per device = 100\n",
      "\u001b[34;1mtextattack\u001b[0m:   Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "\u001b[34;1mtextattack\u001b[0m:   Gradient accumulation steps = 1\n",
      "\u001b[34;1mtextattack\u001b[0m:   Total optimization steps = 280000\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 1\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 1/50\n",
      "Loss 0.57557: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.48it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 67.79%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 82.32%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 2\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 2/50\n",
      "Loss 0.48246: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.48it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 84.03%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 85.27%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 3\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 3/50\n",
      "Loss 0.43032: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.39it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 86.38%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 87.32%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 4\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 4/50\n",
      "Loss 0.39406: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.43it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 88.02%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 88.48%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 5\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 5/50\n",
      "Loss 0.36848: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.36it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 88.88%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 88.96%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 6\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 6/50\n",
      "Loss 0.34971: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.19it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 89.37%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.12%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 7\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 7/50\n",
      "Loss 0.33529: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.02it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 89.68%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.27%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 8\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 8/50\n",
      "Loss 0.32379: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.20it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 89.96%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.40%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 9\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 9/50\n",
      "Loss 0.31438: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.23it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 90.13%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.47%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 10\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 10/50\n",
      "Loss 0.30645: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.18it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 90.31%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.51%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 11\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 11/50\n",
      "Loss 0.29966: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.23it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 90.50%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.45%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 12\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 12/50\n",
      "Loss 0.29377: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.37it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 90.62%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.58%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 13\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 13/50\n",
      "Loss 0.28858: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.29it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 90.77%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.58%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 14\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 14/50\n",
      "Loss 0.28394: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.39it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 90.88%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.62%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 15\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 15/50\n",
      "Loss 0.27977: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.46it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 90.98%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.65%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 16\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 16/50\n",
      "Loss 0.27598: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.35it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.08%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.66%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 17\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 17/50\n",
      "Loss 0.27251: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.38it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.19%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.51%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 18\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 18/50\n",
      "Loss 0.26932: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.51it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.28%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.52%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 19\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 19/50\n",
      "Loss 0.26637: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.18it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.38%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.70%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-15-08-48-546472/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 20\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 20/50\n",
      "Loss 0.26362: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.30it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.46%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.62%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 21\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 21/50\n",
      "Loss 0.26105: 100%|█████████████████████████| 5600/5600 [01:48<00:00, 51.39it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.54%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.57%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 22\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 22/50\n",
      "Loss 0.25863: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.14it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.60%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.60%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 23\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 23/50\n",
      "Loss 0.25636: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.02it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.69%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.59%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 24\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 24/50\n",
      "Loss 0.25420: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.02it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.75%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.53%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 25\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 25/50\n",
      "Loss 0.25216: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.04it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.85%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.37%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 26\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 26/50\n",
      "Loss 0.25022: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.20it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.90%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.56%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 27\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 27/50\n",
      "Loss 0.24836: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.02it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.96%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.42%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 28\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 28/50\n",
      "Loss 0.24658: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.76it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.06%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.41%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 29\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 29/50\n",
      "Loss 0.24487: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 50.92it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.10%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.19%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 30\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 30/50\n",
      "Loss 0.24323: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.81it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.16%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.43%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 31\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 31/50\n",
      "Loss 0.24165: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.86it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.21%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.36%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 32\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 32/50\n",
      "Loss 0.24013: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.89it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.27%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.35%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 33\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 33/50\n",
      "Loss 0.23866: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.59it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.32%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.38%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 34\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 34/50\n",
      "Loss 0.23723: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.86it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.42%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.26%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 35\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 35/50\n",
      "Loss 0.23585: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.54it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.45%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.29%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 36\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 36/50\n",
      "Loss 0.23451: 100%|█████████████████████████| 5600/5600 [01:56<00:00, 48.10it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.51%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.31%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 37\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 37/50\n",
      "Loss 0.23321: 100%|█████████████████████████| 5600/5600 [01:56<00:00, 48.26it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.57%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.24%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 38\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 38/50\n",
      "Loss 0.23193: 100%|█████████████████████████| 5600/5600 [02:10<00:00, 42.97it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.65%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.24%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 39\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 39/50\n",
      "Loss 0.23069: 100%|█████████████████████████| 5600/5600 [02:24<00:00, 38.73it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.69%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.22%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 40\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 40/50\n",
      "Loss 0.22948: 100%|█████████████████████████| 5600/5600 [02:25<00:00, 38.56it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.73%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.21%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 41\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 41/50\n",
      "Loss 0.22831: 100%|█████████████████████████| 5600/5600 [02:25<00:00, 38.47it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.79%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.24%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 42\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 42/50\n",
      "Loss 0.22492: 100%|█████████████████████████| 5600/5600 [02:06<00:00, 44.38it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 92.95%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.15%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 45\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 45/50\n",
      "Loss 0.22384: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.75it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 93.01%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.12%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 46\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 46/50\n",
      "Loss 0.22277: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.84it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 93.06%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.22%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 47\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 47/50\n",
      "Loss 0.22173: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 51.03it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 93.11%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.24%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 48\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 48/50\n",
      "Loss 0.22070: 100%|█████████████████████████| 5600/5600 [01:49<00:00, 50.98it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 93.17%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 88.96%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 49\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 49/50\n",
      "Loss 0.21968: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.60it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 93.22%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.07%\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 50\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 50/50\n",
      "Loss 0.21869: 100%|█████████████████████████| 5600/5600 [01:50<00:00, 50.58it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 93.28%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 89.12%\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote README to ./outputs/2024-11-30-15-08-48-546472/README.md.\n"
     ]
    }
   ],
   "source": [
    "!textattack train --model-name-or-path lstm --per-device-train-batch-size 100 --dataset yelp_polarity --epochs 50 --learning-rate 1e-5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG1J8W6VDm8I"
   },
   "source": [
    "Fine-tune bert-base on the CoLA dataset for 5\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UmRbccgsDmdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mtextattack\u001b[0m: Loading transformers AutoModelForSequenceClassification: bert-base-uncased\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mglue\u001b[0m, subset \u001b[94mcola\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mglue\u001b[0m, subset \u001b[94mcola\u001b[0m, split \u001b[94mvalidation\u001b[0m.\n",
      "\u001b[34;1mtextattack\u001b[0m: Writing logs to ./outputs/2024-11-30-16-51-06-703320/train_log.txt.\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote original training args to ./outputs/2024-11-30-16-51-06-703320/training_args.json.\n",
      "/mnt/aiongpfs/users/agross/venv_iris/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/mnt/aiongpfs/users/agross/venv_iris/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "\u001b[34;1mtextattack\u001b[0m: ***** Running training *****\n",
      "\u001b[34;1mtextattack\u001b[0m:   Num examples = 8551\n",
      "\u001b[34;1mtextattack\u001b[0m:   Num epochs = 5\n",
      "\u001b[34;1mtextattack\u001b[0m:   Num clean epochs = 5\n",
      "\u001b[34;1mtextattack\u001b[0m:   Instantaneous batch size per device = 8\n",
      "\u001b[34;1mtextattack\u001b[0m:   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "\u001b[34;1mtextattack\u001b[0m:   Gradient accumulation steps = 1\n",
      "\u001b[34;1mtextattack\u001b[0m:   Total optimization steps = 5345\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 1\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 1/5\n",
      "Loss 0.54162: 100%|█████████████████████████| 1069/1069 [04:11<00:00,  4.25it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 72.94%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 74.78%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-16-51-06-703320/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 2\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 2/5\n",
      "Loss 0.44871: 100%|█████████████████████████| 1069/1069 [04:11<00:00,  4.26it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 85.18%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 80.35%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-16-51-06-703320/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 3\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 3/5\n",
      "Loss 0.36036: 100%|█████████████████████████| 1069/1069 [04:11<00:00,  4.25it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 93.45%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 81.40%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-16-51-06-703320/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
      "\u001b[34;1mtextattack\u001b[0m: Epoch 4\n",
      "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 4/5\n",
      "Loss 0.24220: 100%|█████████████████████████| 1069/1069 [04:11<00:00,  4.26it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 98.68%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 82.17%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-11-30-16-51-06-703320/best_model/\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote README to ./outputs/2024-11-30-16-51-06-703320/README.md.\n"
     ]
    }
   ],
   "source": [
    "# !textattack train --model bert-base-uncased --dataset glue:cola  --epochs 5\n",
    "!textattack train --per-device-train-batch-size 100 --model-name-or-path bert-base-uncased --dataset glue^cola --per-device-train-batch-size 8 --epochs 5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
